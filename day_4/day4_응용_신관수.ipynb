{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/PEBpung/CNN_wandb/blob/master/WandB_pytorch_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cg1vp_Fe2nFJ"
   },
   "source": [
    "# ğŸš€ Install, Import, and Log In"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "bcFEvH5vhlYw"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ysRzZ5lGh7t5"
   },
   "source": [
    "### 0ï¸âƒ£ Step 0: W&B ì„¤ì¹˜í•˜ê¸°\n",
    "colabì—ì„œ WandBë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ wandb modulì„ install í•´ì•¼í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "8mVVE8w1h8Jc"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install wandb --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ipp9aA2piG-g"
   },
   "source": [
    "### 1ï¸âƒ£ Step 1: W&B ë¡œê·¸ì¸\n",
    "\n",
    "WandBì˜ web ì„œë¹„ìŠ¤ë¥¼ ì´ìš©í•˜ê¸° ìœ„í•´ì„  log inì´ í•„ìš”í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ovzfhTdSiJLI"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkwansu\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fstctcwBiSlH"
   },
   "source": [
    "### 2ï¸âƒ£ Step 2: config ì„¤ì • í›„ `wandb.init` ì •ì˜\n",
    "ì´ì œ ë³¸ê²©ì ìœ¼ë¡œ ëª¨ë¸ì„ í•™ìŠµ ì‹œí‚¤ê¸° ì „ì— ëª‡ê°€ì§€ ì¤€ë¹„ë¥¼ í•˜ë ¤ê³  í•©ë‹ˆë‹¤.  \n",
    "1. hyper-parameter config ì„¤ì •\n",
    "2. model í•™ìŠµì„ ìœ„í•œ pipeline ì •ì˜\n",
    "3. wandb.init()ìœ¼ë¡œ wandb ì„œë²„ì™€ ì—°ê²°\n",
    "4. dataloader ì •ì˜\n",
    "5. model ì •ì˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GyjENuDR5I0J"
   },
   "source": [
    "### 1) config ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "1lGXXFCXiTBN"
   },
   "outputs": [],
   "source": [
    "config  = {\n",
    "    'epochs': 5,\n",
    "    'classes':10,\n",
    "    'batch_size': 128,\n",
    "    'kernels': [16, 32],\n",
    "    'weight_decay': 0.0005,\n",
    "    'learning_rate': 1e-3,\n",
    "    'dataset': 'MNIST',\n",
    "    'architecture': 'CNN',\n",
    "    'seed': 42\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D1lrxavp5Nz3"
   },
   "source": [
    "### 2) model pipeline ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "q8Zjs1lji-uQ"
   },
   "outputs": [],
   "source": [
    "def model_pipeline(hyperparameters):\n",
    "    wandb.init(project='Totoch', entity='kwansu', config=hyperparameters)\n",
    "      \n",
    "    config = wandb.config\n",
    "\n",
    "    model, train_loader, test_loader, criterion, optimizer = make(config)\n",
    "\n",
    "    train(model, train_loader, criterion, optimizer, config)\n",
    "    test(model, test_loader)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "btjji_u27J81"
   },
   "outputs": [],
   "source": [
    "def make(config):\n",
    "    train, test = get_data(train=True), get_data(train=False)\n",
    "    train_loader = make_loader(train, batch_size=config.batch_size)\n",
    "    test_loader = make_loader(test, batch_size=config.batch_size)\n",
    "\n",
    "    model = ConvNet(config.kernels, config.classes).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=config.learning_rate)\n",
    "    \n",
    "    return model, train_loader, test_loader, criterion, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qgsDGEMY5VHi"
   },
   "source": [
    "### 3) dataloader ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Dt1AWh2fxN48"
   },
   "outputs": [],
   "source": [
    "def get_data(slice=5, train=True):\n",
    "    full_dataset = datasets.MNIST(root='./data/MNIST', train=train, \n",
    "                                    download=True,  transform=transforms.ToTensor())\n",
    "    \n",
    "    #  equiv to slicing with [::slice] \n",
    "    sub_dataset = torch.utils.data.Subset(full_dataset, \n",
    "                                          indices=range(0, len(full_dataset), slice))\n",
    "    return sub_dataset\n",
    "\n",
    "\n",
    "def make_loader(dataset, batch_size):\n",
    "    loader = DataLoader(dataset=dataset,\n",
    "                        batch_size=batch_size, \n",
    "                        shuffle=True,\n",
    "                        pin_memory=True, num_workers=2)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E13Qs4Xc5ZGe"
   },
   "source": [
    "### 4) ëª¨ë¸ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "VI1Q0VoQxSUn"
   },
   "outputs": [],
   "source": [
    "# Conventional and convolutional neural network\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, kernels, classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, kernels[0], kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, kernels[1], kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc = nn.Linear(7 * 7 * kernels[-1], classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iB4v24cn2d8o"
   },
   "source": [
    "### 3ï¸âƒ£ Step 3. `wandb.watch`ì™€ `wandb.log`ë¥¼ ì‚¬ìš©í•´ì„œ gradients ì¶”ì í•˜ê¸°\n",
    "- **wandb.watch**ëŠ” gradient, topologyì™€ ê´€ë ¨ëœ ì •ë³´ë¥¼ visualization í•˜ê¸° ìœ„í•œ ì½”ë“œì…ë‹ˆë‹¤.\n",
    "- **wandb.log**ëŠ” visualization í•˜ê³  ì‹¶ì€ ì •ë³´ë¥¼ ë„˜ê²¨ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì´ ë‘ê°€ì§€ ì½”ë“œë¥¼ í™œìš©í•´ì„œ gradientì™€ parameterë¥¼ ì‹œê°í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.   \n",
    "wandb.watchëŠ” í•™ìŠµí•˜ê¸° ì „ trainì˜ ì•ë¶€ë¶„ì— ìœ„ì¹˜ ì‹œì¼œì¤ë‹ˆë‹¤.  \n",
    "wandb.logëŠ” í•™ìŠµ logë¥¼ ì¶œë ¥í•˜ê¸° ë°”ë¡œ ì „ì— metricê³¼ epochì„ ì…ë ¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "9Fz9rw8ExUez"
   },
   "outputs": [],
   "source": [
    "def train(model, loader, criterion, optimizer, config):\n",
    "    wandb.watch(model, criterion, log=\"all\", log_freq=10)\n",
    "\n",
    "    # Run training and track with wandb\n",
    "    total_batches = len(loader) * config.epochs\n",
    "    example_ct = 0  # number of examples seen\n",
    "    batch_ct = 0\n",
    "    for epoch in tqdm(range(config.epochs)):\n",
    "        for batch_idx, (images, labels) in enumerate(loader):\n",
    "\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "    \n",
    "            # Forward pass â¡\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass â¬…\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # Step with optimizer\n",
    "            optimizer.step()\n",
    "\n",
    "            example_ct +=  len(images)\n",
    "\n",
    "            # Report metrics every 25th batch\n",
    "            if ((batch_idx + 1) % 25) == 0:\n",
    "                train_log(loss, example_ct, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ZkSbJ9pLxV4R"
   },
   "outputs": [],
   "source": [
    "def train_log(loss, example_ct, epoch):\n",
    "    loss = float(loss)\n",
    "\n",
    "    # where the magic happens\n",
    "    wandb.log({\"epoch\": epoch, \"loss\": loss}, step=example_ct)\n",
    "    print(f\"Loss after \" + str(example_ct).zfill(5) + f\" examples: {loss:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4wCbKrfA2hv4"
   },
   "source": [
    "### 4ï¸âƒ£ Optional Step 4:  `wandb.save`ë¡œ ì €ì¥í•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "qk-YIDjlxXWK"
   },
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        correct, total = 0, 0\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            pred = outputs.max(1, keepdim = True)[1]                                       \n",
    "            correct += pred.eq(labels.view_as(pred)).sum().item() \n",
    "\n",
    "            total = len(test_loader.dataset)\n",
    "\n",
    "        print(f\"Accuracy of the model on the {total} \" +\n",
    "              f\"test images: {100 * correct / total}%\")\n",
    "        \n",
    "        wandb.log({\"test_accuracy\": correct / total})\n",
    "\n",
    "    # Save the model in the exchangeable ONNX format\n",
    "    torch.onnx.export(model, images, \"model.onnx\")\n",
    "    wandb.save(\"model.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76MuW0uc50QA"
   },
   "source": [
    "## ğŸƒâ€â™€ï¸ Run training and watch your metrics live on [wandb.ai](https://wandb.ai)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "kOSFlQBd5vJN"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kwansu/Totoch/runs/2npl4utd\" target=\"_blank\">neat-silence-1</a></strong> to <a href=\"https://wandb.ai/kwansu/Totoch\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e161c08865394aee84a76b15ca4232c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "191c5185e9bc415285baf18ff07c5e0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43fe8ec537f54990aa47644e94b937f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f40fa2795e4e4aca96f950f8b194aea0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc6e834266d24f4ea8e8e3ff0dd8f732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 03200 examples: 0.842\n",
      "Loss after 06400 examples: 0.511\n",
      "Loss after 09600 examples: 0.320\n",
      "Loss after 15200 examples: 0.321\n",
      "Loss after 18400 examples: 0.194\n",
      "Loss after 21600 examples: 0.185\n",
      "Loss after 27200 examples: 0.135\n",
      "Loss after 30400 examples: 0.148\n",
      "Loss after 33600 examples: 0.159\n",
      "Loss after 39200 examples: 0.218\n",
      "Loss after 42400 examples: 0.074\n",
      "Loss after 45600 examples: 0.137\n",
      "Loss after 51200 examples: 0.020\n",
      "Loss after 54400 examples: 0.070\n",
      "Loss after 57600 examples: 0.052\n",
      "Accuracy of the model on the 2000 test images: 96.8%\n"
     ]
    }
   ],
   "source": [
    "model = model_pipeline(config)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "WandB_pytorch_tutorial.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "interpreter": {
   "hash": "93f0d9e47ee3596f3a4c40963a5f80a2a8195902cfa23a0f0d123dcd43c69f1e"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
